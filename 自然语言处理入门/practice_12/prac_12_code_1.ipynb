{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 自然语言处理\n",
    "\n",
    "自然语言是指汉语、英语、法语等人们日常使用的语言，自然语言处理是指用计算机对自然语言的形、音、义等信息进行处理，即对字、词、句、篇章等进行输入、输出、识 别、分析、理解等一系列操作的过程。\n",
    "\n",
    "根据研究对象的不同，自然语言处理可以分为词法分析、句法分析和语义分析等内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 词语切分(word tokenization)\n",
    "\n",
    "即分词，是将句子切分成单词的过程。句子是单词的集合，对句子进行词语切分，本质上就是将一个句子分割成一个单词列表，该单词列表又可以重新还原为原句子。\n",
    "\n",
    "- 采用jiaba模块进行中文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我/是/一名/大学生/，/我/喜欢/自然语言/处理/。\n"
     ]
    }
   ],
   "source": [
    "#安装jieba\n",
    "# !pip install jieba -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "\n",
    "import jieba\n",
    "seg_list = jieba.cut(\"我是一名大学生，我喜欢自然语言处理。\") \n",
    "print(\"/\".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 采用NLTK模块进行英文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: nltk in e:\\users\\admin\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in e:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "#安装nltk\n",
    "!pip install nltk -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "showing info http://nltk.org/nltk_data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download()\n",
    "\n",
    "from nltk import data\n",
    "data.path.append(r'F:\\BaiduNetdiskDownload\\nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'a', 'college', 'student', '.', 'I', 'love', 'natural', 'language', 'processing', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"I am a college student. I love natural language processing.\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 词性标注（part-of-speech tagging）\n",
    "\n",
    "词性（part of speech, POS）是基于语法语境和词语作用的具体词汇分类，是词语的基本语法属性；\n",
    "\n",
    "词性标注，又称为词类标注或简称为标注，是指为分词结果中的每个单词标注一个正确的词性的过程，即确定每个词是名词、动词、形容词或者其他词性的过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\admin\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 2.794 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 r\n",
      "是 v\n",
      "一名 m\n",
      "大学生 n\n",
      "， x\n",
      "我 r\n",
      "喜欢 v\n",
      "自然语言 l\n",
      "处理 v\n",
      "。 x\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"我是一名大学生，我喜欢自然语言处理。\") \n",
    "for word, flag in words:\n",
    "    print('%s %s' % (word, flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 词义消歧\n",
    "\n",
    "词义消歧指的是确定待分析词语在文本中的含义的过程。词义消歧在文本理解的任务中极为重要，是句子和篇章语义理解的基础。\n",
    "\n",
    "- 以基于Lesk算法的词义消歧工具pywsd为例，展示词义消歧具体过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pywsd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-75353c62dd64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpywsd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlesk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msimple_lesk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'I went to the bank to deposit my money.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mambiguous\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'bank'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple_lesk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mambiguous\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pywsd'"
     ]
    }
   ],
   "source": [
    "from pywsd.lesk import simple_lesk\n",
    "sent = 'I went to the bank to deposit my money.'\n",
    "ambiguous = 'bank'\n",
    "answer = simple_lesk(sent, ambiguous, pos='n')\n",
    "print(answer)\n",
    "print(answer.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 自然语言处理常用工具\n",
    "\n",
    "### 10.5.1 NLTK  <font color=Blue>(Self-learning)</font>\n",
    "\n",
    "NLTK 大概是最知名的Python自然语言处理工具了，全称\"Natural Language Toolkit\", 诞生于宾夕法尼亚大学，以研究和教学为目的而生，因此也特别适合入门学习。NLTK虽然主要面向英文，但是它的很多NLP模型或者模块是语言无关的，因此如果某种语言有了初步的Tokenization或者分词，NLTK的很多工具包是可以复用的。\n",
    "\n",
    "关于NLTK，网上已经有了很多介绍资料，当然首推的NLTK学习资料依然是官方出的在线书籍 [NLTK Book：Natural Language Processing with Python – Analyzing Text with the Natural Language Toolkit ](http://www.nltk.org/book/)，目前基于Python 3 和 NLTK 3 ，可以在线免费阅读和学习。\n",
    "\n",
    "请阅读本小节代码，**<font color=red>该部分代码不用运行！</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk是一个功能很强大的第三方自然语言处理模块，集合了很多开发者的工具（resources）。 采用pip install nltk安装的只是nltk的基础包。如果用它做分词、词性标注、命名实体等功能，需要额外下载各种工具。\n",
    "\n",
    "如下是采用代码下载。 但由于下载源是国外服务器，非常慢，甚至失败。\n",
    "```\n",
    "import nltk\n",
    "nltk.download()\n",
    "```\n",
    "\n",
    "**解决方案**：\n",
    "1. 大家下载QQ群文件里的nltk_data.zip文件，然后解压到某个目录。 解压时注意不要有两层nltk_data路径\n",
    "2. 执行如下代码：\n",
    "```\n",
    "from nltk import data\n",
    "data.path.append(r'F:\\BaiduNetdiskDownload\\nltk_data') ##该路径需要改为自己的nltk_data路径\n",
    "```\n",
    "执行完，即可运行相关程序了。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import data\n",
    "data.path.append(r'F:\\BaiduNetdiskDownload\\nltk_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 句子切分（sentence tokenization）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Adam, how are you?', 'I hope everything is going well.', 'Today is a good day, see you dude.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"Hello Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词语切分（word tokenization）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Adam', ',', 'how', 'are', 'you', '?', 'I', 'hope', 'everything', 'is', 'going', 'well', '.', 'Today', 'is', 'a', 'good', 'day', ',', 'see', 'you', 'dude', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Hello Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词干提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "print(porter_stemmer.stem('working'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('works'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('playing',pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'does', 'the', 'fox', 'say']\n",
      "[('what', 'WDT'), ('does', 'VBZ'), ('the', 'DT'), ('fox', 'NNS'), ('say', 'VBP')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text=nltk.word_tokenize('what does the fox say')\n",
    "print(text)\n",
    "print(nltk.pos_tag(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5.2 jieba\n",
    "\n",
    "jieba库是一款优秀的 Python 第三方中文分词库，支持三种分词模式：精确模式、全模式和搜索引擎模式，下面是三种模式的特点。\n",
    "\n",
    "- 精确模式：试图将语句最精确的切分，不存在冗余数据，适合做文本分析\n",
    "\n",
    "- 全模式：将语句中所有可能是词的词语都切分出来，速度很快，但是存在冗余数据\n",
    "\n",
    "- 搜索引擎模式：在精确模式的基础上，对长词再次进行切分\n",
    "\n",
    "**<font color=red>请阅读并运行本小节代码。</font>**\n",
    "\n",
    "#### 1. jieba分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\Brett\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.253 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "默认模式:  我/是/一名/武汉大学/的/学生\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "seg_list = jieba.cut(\"我是一名武汉大学的学生\")  # 使用默认模式，默认是精确模式\n",
    "print(\"默认模式: \",\"/\".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 利用Jieba进行关键词提取\n",
    "\n",
    "关键词提取就是从文本里面把跟这篇文章意义最相关的一些词语抽取出来，在文献检索、自动文摘、文本聚类/分类等方面有着重要的应用。\n",
    "\n",
    "关键词提取算法一般分为有监督和无监督两类\n",
    "\n",
    "有监督的关键词提取方法主要是通过分类的方式进行，通过构建一个较为丰富和完善的词表，然后判断每个文档与词表中每个词的匹配程度，以类似打标签的方式，达到关键词提取的效果。优点是精度较高，缺点是需要大批量的标注数据，人工成本过高，并且词表需要及时维护。\n",
    "\n",
    "相比较而言，无监督的方法对数据的要求低，既不需要一张人工生成，维护的词表，也不需要人工标注语料辅助训练。目前比较常用的关键词提取算法都是基于无监督算法。如TF-IDF算法，TextRank算法和主题模型算法（包括LSA，LSI，LDA等）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于 TF-IDF 算法的关键词提取\n",
    "\n",
    "TF-IDF是一种数值统计方法，用于反映一个词对于预料中某篇文档的重要性，它的主要思想为：如果某个词在一篇文档中出现的频率高，即TF高；并且在其他文档中很少出现，即IDF高，则认为这个词具有很好的类别区分能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我校 0.3650310687908397\n",
      "自强 0.36010570737862596\n",
      "弘毅 0.3397889749526718\n",
      "校训 0.2773034698328244\n",
      "拓新 0.18251553439541984\n",
      "语出 0.17087980841068703\n",
      "求是 0.16727081943206107\n",
      "自强不息 0.16308094392519082\n",
      "武汉大学 0.14590644626137403\n",
      "中华民族 0.12336785834534353\n",
      "含义 0.11406043013648855\n",
      "伟大 0.1062369610119084\n",
      "不断进取 0.10082084329312978\n",
      "明诚 0.09772568979618321\n",
      "天行健 0.09552964344198472\n",
      "奋发向上 0.09382625755343511\n",
      "修学 0.09382625755343511\n",
      "好古 0.09382625755343511\n",
      "传统美德 0.0924344899442748\n",
      "校风 0.0924344899442748\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "sentence = '1993年，在广泛征求各方面意见的基础上，经校务委员会审议，武汉大学新校训定为：\\\n",
    "自强 弘毅 求是 拓新。“自强”语出《周易》“天行健、君子以自强不息”。意为自尊自重，不断自力图强，\\\n",
    "奋发向上。自强是中华民族的传统美德，成就事业当以此为训。我校最早前身为“自强学堂”，其名也取此意。\\\n",
    "“弘毅”出自《论语》“士不可以不弘毅，任重而道远”一语。意谓抱负远大，坚强刚毅。\\\n",
    "我校30年代校训“明诚弘毅”就含此一词。用“自强”、“弘毅”，既概括了上述含义，\\\n",
    "又体现了我校的历史纵深与校风延续。“求是”即为博学求知，努力探索规律，追求真理。\\\n",
    "语出《汉书》“修学好古，实事求是”。“拓新”，意为开拓、创新，不断进取。概言之，\\\n",
    "我校新校训的整体含义是： 继承和发扬中华民族自强不息的伟大精神，树立为国家的繁荣昌盛刻苦学习、\\\n",
    "积极奉献的伟大志向，以坚毅刚强的品格和科学严谨的治学态度，努力探求事物发展的客观规律，开创新局面，\\\n",
    "取得新成绩，办好社会主义的武汉大学，不断为国家作出新贡献。'\n",
    "\n",
    "keywords=jieba.analyse.extract_tags(sentence,topK=20,withWeight=True,allowPOS=())\n",
    "for item in keywords:\n",
    "    print(item[0],item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于 TextRank 算法的关键词提取\n",
    "\n",
    "此种算法的一个重要特点是可以脱离语料库的背景，仅对单篇文档进行分析就可以提取该文档的关键词。基本思想来源于Google的PageRank算法。这种算法是1997年，Google创始人拉里.佩奇和谢尔盖.布林在构建早期的搜索系统原型时提出的一种链接分析算法，基本思想有两条：\n",
    "\n",
    "    - 1）链接数量。一个网页被越多的其他网页链接，说明这个网页越重要\n",
    "    - 2）链接质量。一个网页被一个越高权值的网页链接，也能表明这个网页越重要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "国家 1.0\n",
      "含义 0.9514975766301668\n",
      "校训 0.8831816220202372\n",
      "创新 0.7692786654850344\n",
      "客观规律 0.6586080643543306\n",
      "探求 0.6580889640584097\n",
      "历史 0.5969396515025892\n",
      "自强 0.591162600642535\n",
      "成绩 0.578783632054623\n",
      "取得 0.5695284955354087\n",
      "求知 0.5629593666958073\n",
      "探索 0.5608546033210623\n",
      "态度 0.5597483134160578\n",
      "奉献 0.5595424204402842\n",
      "局面 0.5572237414652758\n",
      "事物 0.5203076902458431\n",
      "校务 0.5201438592373493\n",
      "方面 0.5199791277209296\n",
      "审议 0.5190906744439646\n",
      "意见 0.5181603419662032\n"
     ]
    }
   ],
   "source": [
    "keywords = jieba.analyse.textrank(sentence, topK=20, withWeight=True, \\\n",
    "                                  allowPOS=('ns','n','vn','v'))\n",
    "for item in keywords:\n",
    "    print(item[0],item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5.3 pyLTP  <font color=Blue>(Self-learning)</font>\n",
    "\n",
    "语言技术平台（Languange Technolog Platform, LTP）是由哈工大社会计算与信息检索中心研发的自然语言处理工具。提供了一系列中文自然语言处理工具，用户可以使用这些工具对于中文文本进行分词、词性标注、句法分析等等工作。从应用角度来看，LTP为用户提供了下列组件：\n",
    "\n",
    "- 针对单一自然语言处理任务，生成统计机器学习模型的工具\n",
    "- 针对单一自然语言处理任务，调用模型进行分析的编程接口\n",
    "- 使用流水线方式将各个分析工具结合起来，形成一套统一的中文自然语言处理系统\n",
    "- 系统可调用的，用于中文语言处理的模型文件\n",
    "- 针对单一自然语言处理任务，基于云端的编程接口\n",
    "\n",
    "pyltp 是 LTP 的 Python 封装，提供了分词，词性标注，命名实体识别，依存句法分析，语义角色标注的功能。\n",
    "\n",
    "请阅读本小节代码，**<font color=red>该部分代码不用运行！</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 利用PYLTP进行分句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我喜欢自然语言处理。\n",
      "我也是！\n"
     ]
    }
   ],
   "source": [
    "from pyltp import SentenceSplitter\n",
    "sents = SentenceSplitter.split('我喜欢自然语言处理。我也是！')\n",
    "print ('\\n'.join(sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 利用PYLTP进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我\t喜欢\t自然\t语言\t处理\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "LTP_DATA_DIR = 'D:\\LTP\\ltp_data'  # ltp模型目录的路径\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  #分词模型路径，模型名称为`cws.model`\n",
    "from pyltp import Segmentor\n",
    "segmentor = Segmentor()  # 初始化实例\n",
    "segmentor.load(cws_model_path)  # 加载模型\n",
    "words = segmentor.segment('我喜欢自然语言处理')  # 分词\n",
    "print ('\\t'.join(words))\n",
    "segmentor.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 利用PYLTP进行词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r\tv\tn\tn\tv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "LTP_DATA_DIR = 'D:\\LTP\\ltp_data'      # ltp模型目录的路径\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  \n",
    "# 词性标注模型路径，模型名称为`pos.model`\n",
    "from pyltp import Postagger\n",
    "postagger = Postagger() # 初始化实例\n",
    "postagger.load(pos_model_path)  # 加载模型\n",
    "words = ['我', '喜欢', '自然', '语言','处理']  # 分词结果\n",
    "postags = postagger.postag(words)  # 词性标注\n",
    "print ('\\t'.join(postags))\n",
    "postagger.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 利用PYLTP进行命名实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\tO\tO\tO\tO\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "LTP_DATA_DIR = 'D:\\LTP\\ltp_data'  # ltp模型目录的路径\n",
    "ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')  # 命名实体识别模型路径，模型名称为`pos.model`\n",
    "from pyltp import NamedEntityRecognizer\n",
    "recognizer = NamedEntityRecognizer() # 初始化实例\n",
    "recognizer.load(ner_model_path)  # 加载模型\n",
    "words = ['我', '喜欢', '自然', '语言','处理']\n",
    "postags = ['r', 'v','n','n', 'v']\n",
    "netags = recognizer.recognize(words, postags)  # 命名实体识别\n",
    "print ('\\t'.join(netags))\n",
    "recognizer.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 利用PYLTP进行依存句法分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2:SBV\t0:HED\t4:ATT\t5:ATT\t2:VOB\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "LTP_DATA_DIR = 'D:\\LTP\\ltp_data'  # ltp模型目录的路径\n",
    "par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  # 依存句法分析模型路径，模型名称为`parser.model`\n",
    "from pyltp import Parser\n",
    "parser = Parser() # 初始化实例\n",
    "parser.load(par_model_path)  # 加载模型\n",
    "words = ['我', '喜欢', '自然', '语言','处理']\n",
    "postags = ['r', 'v','n','n', 'v']\n",
    "arcs = parser.parse(words, postags)  # 句法分析\n",
    "print (\"\\t\".join(\"%d:%s\" % (arc.head, arc.relation) for arc in arcs))\n",
    "parser.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 利用PYLTP进行语义角色标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 A0:(0,0)A1:(2,4)\n",
      "4 A1:(2,3)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "LTP_DATA_DIR = 'D:\\LTP\\ltp_data'  # ltp模型目录的路径\n",
    "srl_model_path = os.path.join(LTP_DATA_DIR, 'pisrl_win.model')  # 语义角色标注模型目录路径，模型目录为`srl`。\n",
    "from pyltp import SementicRoleLabeller\n",
    "labeller = SementicRoleLabeller() # 初始化实例\n",
    "labeller.load(srl_model_path)  # 加载模型\n",
    "words = ['我', '喜欢', '自然', '语言','处理']\n",
    "postags = ['r', 'v','n','n', 'v']\n",
    "# arcs 使用依存句法分析的结果\n",
    "roles = labeller.label(words, postags, arcs)  # 语义角色标注\n",
    "# 打印结果\n",
    "for role in roles:\n",
    "    print (role.index, \"\".join(\n",
    "        [\"%s:(%d,%d)\" % (arg.name, arg.range.start, arg.range.end) for arg in role.arguments]))\n",
    "labeller.release()  # 释放模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 上机实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 利用正则表达式，将“i am a college student, I am not a businessman.”中拼写错误的“i” 替换为“I”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a college student, I love whu. \n"
     ]
    }
   ],
   "source": [
    "text= \" i am a college student, i love whu. \"\n",
    "import re\n",
    "pattern = re.compile(r'(?:[^\\w]|\\b)i(?:[^\\w])')\n",
    "while True:\n",
    "    result = pattern.search(text)\n",
    "    if result:\n",
    "        if result.start(0) != 0:\n",
    "            text= text[:result.start(0)+1]+'I'+ text[result.end(0)-1:]\n",
    "        else:\n",
    "            text= text [:result.start(0)]+'I'+ text[result.end(0)-1:]\n",
    "    else:\n",
    "        break\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 句子“I love love wuhan university”中有单词重复的错误，利用正则表达式检查重复的单词并只保留一个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I love wuhan university.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = ' I love love wuhan university.'\n",
    "pattern = re.compile(r'\\b(\\w+)(\\s+\\1){1,}\\b')\n",
    "matchResult = pattern.search(text)\n",
    "text = pattern.sub(matchResult.group(1), text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 利用 Jieba 分词对此句子进行分词和词性标注:“我是一名大学生，我来自武汉大学”。 学生可执行设计需要进行实验的文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Brett\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.945 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 r\n",
      "是 v\n",
      "一名 m\n",
      "大学生 n\n",
      "， x\n",
      "我 r\n",
      "来自 v\n",
      "武汉大学 nt\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"我是一名大学生，我来自武汉大学\") \n",
    "for word, flag in words:\n",
    "    print('%s %s' % (word, flag))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
